<h3>Commission Assessment</h3>

<p>Commission errors are the result of falsely declaring a polygon to be a member of some class. In a multi-class thematic map, every error is a commission error for one class and an omission error for another. In change detection, change is initially the only variable so commission and omission really only have one definition. Commission errors, erroneous inclusions, would be locations mapped as change which did not change during the time-period of observation. Omission errors are the opposite, i.e., locations that changed that were not mapped as change. The RF statistical model provided a designation for each polygon in the study area as either changed or non-changed. Statistical models are designed to minimize overall error so with two classes, the minimum error would result from assigning the most likely class to each unobserved polygon. This partitions the commission and omission errors into two different fractions. All commission errors reside with the polygons labeled as change and all the omission errors reside with the polygons labeled as no change. Therefore, to eliminate commission errors, an analyst needs to “check the computer’s work.” The goal of this analysis was to eliminate as much error as possible with minimal effort, i.e. maximizing the efficiency of the change detection. Since a target polygon must be observed to validate its thematic accuracy, the model was set up to capture as much error as possible in a relatively small area. With change detection, the number of polygons labeled as change is generally much smaller than those labeled as no-change, so the task of eliminating commission error is much smaller than trying to eliminate omission error. The AAViewer was used to review predicted polygons. Assuming half the error was commission error, reviewing all commission polygons should eliminate half of the model error while only reviewing a small portion of the study area (usually &lt 5%). This approach was a hybrid method of regular predictive statistical modeling and analyst driven photo interpretation. The statistical modeling phase was used to focus the analyst’s effort on those areas most likely to exhibit the target change events. In this way, the predicted change areas were interpreted by an analyst and all mapped change polygons are effectively derived from photo interpretation [23,46]. Also, the RF procedure provided a probability of change for each polygon. From the simple classification standpoint, the area of predicted change was made up of those polygons which were most likely change, i.e. those with a probability of change greater than 0.5. If the acceptance threshold for classifying change is lowered, more of the overall error will be pushed into the smaller change fraction. For example if the acceptance threshold is lowered from 0.5 to 0.4 the number of polygons that must be reviewed increases but now ~60% of the error is eliminated. Effectively the overall project error rate has been lowered by 10% by reviewing the extra polygons. As criteria are decreased, additional effort to eliminate error rises exponentially. Therefore some optimal acceptance criteria should exist beyond which the additional effort is not worth the increase in accuracy.  At this point the balancing of the criteria is purely subjective and should be informed by the research question. Ideally one would be able to assign a value to mapping each individual change event and compare it to the cost of an analyst’s time to find that event and create a stopping rule when they are equal. At high change probabilities, an analyst may find every reviewed polygon is a change. At low probability frequencies an analyst may need to review 10-20 polygons to find a single change event. As such the cost to find additional change continues to increase as polygons with smaller change probabilities are reviewed. </p>